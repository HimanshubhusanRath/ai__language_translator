{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "designed-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Bidirectional \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "\n",
    "SAMPLE_SIZE = 10000\n",
    "LATENT_DIMS = 256\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tribal-capital",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "official-freight",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = None\n",
    "\n",
    "with open('dataset/deu.txt', encoding='utf-8') as file:\n",
    "    lines = file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cellular-outline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the input texts, output texts\n",
    "# Find the input characters and output characters\n",
    "input_texts = []\n",
    "output_texts = []\n",
    "input_chars = set()\n",
    "output_chars = set()\n",
    "\n",
    "for line in lines[: min(SAMPLE_SIZE, len(lines)-1)]:\n",
    "    words = line.split('\\t')\n",
    "    english = words[0]\n",
    "    german = '\\t' + words[1] + '\\n'\n",
    "\n",
    "    input_texts.append(english)\n",
    "    output_texts.append(german)\n",
    "\n",
    "    # Add the characters present in the words to the character list\n",
    "    input_chars.update(list(english))\n",
    "    output_chars.update(list(german))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "developed-grenada",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_chars = sorted(list(input_chars))\n",
    "output_chars = sorted(list(output_chars))\n",
    "\n",
    "num_encoder_tokens = len(input_chars)\n",
    "num_decoder_tokens = len(output_chars)\n",
    "\n",
    "max_encoder_seq_len = max([len(x) for x in input_texts])\n",
    "max_decoder_seq_len = max([len(x) for x in output_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "unauthorized-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input texts : 10000\n",
      "Number of encoder tokens : 70\n",
      "Number of decoder tokens : 85\n",
      "Maximum number of input sequences : 15\n",
      "Maximum number of output sequences : 51\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of input texts : {len(input_texts)}')\n",
    "print(f'Number of encoder tokens : {num_encoder_tokens}')\n",
    "print(f'Number of decoder tokens : {num_decoder_tokens}')\n",
    "print(f'Maximum number of input sequences : {max_encoder_seq_len}')\n",
    "print(f'Maximum number of output sequences : {max_decoder_seq_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "insured-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input and output tokens to indexed form\n",
    "input_chars_index = dict([(char,i) for (i,char) in enumerate(input_chars) ])\n",
    "output_chars_index = dict([(char,i) for (i,char) in enumerate(output_chars) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-somerset",
   "metadata": {},
   "source": [
    "### Prepare the input data for the model (3-dimensional data)\n",
    "* 'encoder_input_data' is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "* 'decoder_input_data' is a 3D array of shape (num_pairs, max_french_sentence_length, num_french_characters) containg a one-hot vectorization of the French sentences.\n",
    "* 'decoder_target_data' is the same as decoder_input_data but offset by one timestep. decoder_target_data[:, t, :] will be the same as decoder_input_data[:, t + 1, :]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "harmful-amino",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((\n",
    "                        len(input_texts),\n",
    "                        max_encoder_seq_len,\n",
    "                        num_encoder_tokens\n",
    "                     ), dtype=np.float32)\n",
    "\n",
    "decoder_input_data = np.zeros((\n",
    "                        len(output_texts),\n",
    "                        max_decoder_seq_len,\n",
    "                        num_decoder_tokens\n",
    "                     ), dtype=np.float32)\n",
    "\n",
    "decoder_output_data = np.zeros((\n",
    "                        len(output_texts),\n",
    "                        max_decoder_seq_len,\n",
    "                        num_decoder_tokens\n",
    "                     ), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "medical-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 15, 70), (10000, 51, 85), (10000, 51, 85))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape, decoder_input_data.shape, decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "immune-schema",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_SPACE = ' '\n",
    "\n",
    "# Fill the content in these empty (zero value) arrays\n",
    "''' \n",
    "LOGIC:\n",
    "1. All the positions are initialized to 0 by default.\n",
    "2. Update the positions for the characters present in the text as 1.\n",
    "3. Fill the rest of the positions (position for which there is no character available to fill) with SPACE\n",
    "    e.g. if the max_encoder_seq_len = 10 and there are only 6 characters in the input, we need to fill the other \n",
    "       4 positions with SPACE character.\n",
    "'''\n",
    "\n",
    "for i,(inp, out) in enumerate(zip(input_texts, output_texts)):\n",
    "    # Prepare the input data\n",
    "    for t,char in enumerate(inp):\n",
    "        encoder_input_data[i,t,input_chars_index[char]] = 1\n",
    "    encoder_input_data[i,t+1:,input_chars_index[CHAR_SPACE]] = 1\n",
    "    \n",
    "    # Prepare the output data\n",
    "    for t,char in enumerate(out):\n",
    "        decoder_input_data[i,t,output_chars_index[char]] = 1\n",
    "        \n",
    "        # for decoder_output_data: This data is only one time step ahead of the decoder_input_data and \n",
    "        # the START character is not included here.\n",
    "        if t>0:\n",
    "            decoder_output_data[i,t-1,output_chars_index[char]] = 1\n",
    "        \n",
    "    decoder_input_data[i,t+1:,output_chars_index[CHAR_SPACE]] = 1\n",
    "    decoder_output_data[i,t:,output_chars_index[CHAR_SPACE]] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "judicial-boutique",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n",
    "### 1. Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "stone-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "ENC_IP = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(LATENT_DIMS, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(ENC_IP)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "ENC_STATE = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-munich",
   "metadata": {},
   "source": [
    "### 2. Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "premier-distance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `ENC_STATE` as initial state.\n",
    "DEC_IP = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the \n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(LATENT_DIMS, return_sequences=True, return_state=True)\n",
    "DEC_OP, _, _ = decoder_lstm(DEC_IP,initial_state=ENC_STATE)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "DEC_OUT = decoder_dense(DEC_OP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-earth",
   "metadata": {},
   "source": [
    "## Define the model & Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "detailed-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([ENC_IP, DEC_IP], DEC_OUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "continued-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "downtown-authentication",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/20\n",
      "8000/8000 [==============================] - 73s 9ms/sample - loss: 1.3048 - accuracy: 0.6890 - val_loss: 1.2144 - val_accuracy: 0.6776\n",
      "Epoch 2/20\n",
      "8000/8000 [==============================] - 70s 9ms/sample - loss: 0.8902 - accuracy: 0.7616 - val_loss: 0.9395 - val_accuracy: 0.7400\n",
      "Epoch 3/20\n",
      "8000/8000 [==============================] - 72s 9ms/sample - loss: 0.7247 - accuracy: 0.8022 - val_loss: 0.8190 - val_accuracy: 0.7720\n",
      "Epoch 4/20\n",
      "8000/8000 [==============================] - 80s 10ms/sample - loss: 0.6338 - accuracy: 0.8194 - val_loss: 0.7535 - val_accuracy: 0.7879\n",
      "Epoch 5/20\n",
      "8000/8000 [==============================] - 81s 10ms/sample - loss: 0.5784 - accuracy: 0.8334 - val_loss: 0.7023 - val_accuracy: 0.7987\n",
      "Epoch 6/20\n",
      "8000/8000 [==============================] - 83s 10ms/sample - loss: 0.5390 - accuracy: 0.8445 - val_loss: 0.6723 - val_accuracy: 0.8054\n",
      "Epoch 7/20\n",
      "8000/8000 [==============================] - 81s 10ms/sample - loss: 0.5068 - accuracy: 0.8532 - val_loss: 0.6457 - val_accuracy: 0.8141\n",
      "Epoch 8/20\n",
      "8000/8000 [==============================] - 82s 10ms/sample - loss: 0.4808 - accuracy: 0.8607 - val_loss: 0.6182 - val_accuracy: 0.8236\n",
      "Epoch 9/20\n",
      "8000/8000 [==============================] - 82s 10ms/sample - loss: 0.4573 - accuracy: 0.8677 - val_loss: 0.6195 - val_accuracy: 0.8208\n",
      "Epoch 10/20\n",
      "8000/8000 [==============================] - 82s 10ms/sample - loss: 0.4373 - accuracy: 0.8730 - val_loss: 0.5957 - val_accuracy: 0.8299\n",
      "Epoch 11/20\n",
      "8000/8000 [==============================] - 84s 11ms/sample - loss: 0.4199 - accuracy: 0.8774 - val_loss: 0.5796 - val_accuracy: 0.8336\n",
      "Epoch 12/20\n",
      "8000/8000 [==============================] - 87s 11ms/sample - loss: 0.4027 - accuracy: 0.8829 - val_loss: 0.5740 - val_accuracy: 0.8354\n",
      "Epoch 13/20\n",
      "8000/8000 [==============================] - 77s 10ms/sample - loss: 0.3874 - accuracy: 0.8871 - val_loss: 0.5621 - val_accuracy: 0.8398\n",
      "Epoch 14/20\n",
      "8000/8000 [==============================] - 75s 9ms/sample - loss: 0.3733 - accuracy: 0.8915 - val_loss: 0.5558 - val_accuracy: 0.8408\n",
      "Epoch 15/20\n",
      "8000/8000 [==============================] - 77s 10ms/sample - loss: 0.3594 - accuracy: 0.8955 - val_loss: 0.5481 - val_accuracy: 0.8446\n",
      "Epoch 16/20\n",
      "8000/8000 [==============================] - 79s 10ms/sample - loss: 0.3468 - accuracy: 0.8991 - val_loss: 0.5446 - val_accuracy: 0.8461\n",
      "Epoch 17/20\n",
      "8000/8000 [==============================] - 78s 10ms/sample - loss: 0.3344 - accuracy: 0.9025 - val_loss: 0.5387 - val_accuracy: 0.8479\n",
      "Epoch 18/20\n",
      "8000/8000 [==============================] - 78s 10ms/sample - loss: 0.3228 - accuracy: 0.9062 - val_loss: 0.5330 - val_accuracy: 0.8503\n",
      "Epoch 19/20\n",
      "8000/8000 [==============================] - 79s 10ms/sample - loss: 0.3118 - accuracy: 0.9096 - val_loss: 0.5331 - val_accuracy: 0.8507\n",
      "Epoch 20/20\n",
      "8000/8000 [==============================] - 77s 10ms/sample - loss: 0.3013 - accuracy: 0.9128 - val_loss: 0.5374 - val_accuracy: 0.8511\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f16e0850>"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data], \n",
    "    decoder_output_data, \n",
    "    epochs=EPOCHS, \n",
    "    batch_size = BATCH_SIZE, \n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-cross",
   "metadata": {},
   "source": [
    "## Inferencing\n",
    "\n",
    "Let's generate the output sequence using the trained model\n",
    "\n",
    "#### Define the model for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "contained-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Encoder model\n",
    "encoder_model = Model(ENC_IP, ENC_STATE)\n",
    "\n",
    "# Define the Decoder model\n",
    "DEC_STATE_IP_H = Input(shape=(LATENT_DIMS,))\n",
    "DEC_STATE_IP_C = Input(shape=(LATENT_DIMS,))\n",
    "DEC_STATES_inputs = [DEC_STATE_IP_H, DEC_STATE_IP_C]\n",
    "\n",
    "DEC_OUT, state_h, state_c = decoder_lstm(DEC_IP, initial_state=DEC_STATES_inputs)\n",
    "DEC_STATES = [state_h, state_c]\n",
    "\n",
    "DEC_OUT = decoder_dense(DEC_OUT)\n",
    "decoder_model = Model(\n",
    "    [DEC_IP] + DEC_STATES_inputs,\n",
    "    [DEC_OUT] + DEC_STATES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "logical-enough",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([None, None, 85])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEC_OUT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "activated-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse lookup token index to decide sequences back to something readable\n",
    "reverse_input_char_index = dict([(i, char) for (char,i) in input_chars_index.items() ])\n",
    "reverse_output_char_index = dict([(i,char) for (char,i) in output_chars_index.items() ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "guided-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, output_chars_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_output_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "distributed-university",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input seq : Go.\n",
      "Output seq : Verschwinde!\n",
      "\n",
      "Input seq : Hi.\n",
      "Output seq : Halle Tom!\n",
      "\n",
      "Input seq : Hi.\n",
      "Output seq : Halle Tom!\n",
      "\n",
      "Input seq : Run!\n",
      "Output seq : Halle dich!\n",
      "\n",
      "Input seq : Run.\n",
      "Output seq : Fangt Tom!\n",
      "\n",
      "Input seq : Wow!\n",
      "Output seq : Hol zu schnell!\n",
      "\n",
      "Input seq : Wow!\n",
      "Output seq : Hol zu schnell!\n",
      "\n",
      "Input seq : Duck!\n",
      "Output seq : Halte Tom!\n",
      "\n",
      "Input seq : Fire!\n",
      "Output seq : Werfen Sie Tom!\n",
      "\n",
      "Input seq : Help!\n",
      "Output seq : Halle!\n",
      "\n",
      "Input seq : Help!\n",
      "Output seq : Halle!\n",
      "\n",
      "Input seq : Stay.\n",
      "Output seq : Bleib das hin!\n",
      "\n",
      "Input seq : Stop!\n",
      "Output seq : Halte das!\n",
      "\n",
      "Input seq : Stop!\n",
      "Output seq : Halte das!\n",
      "\n",
      "Input seq : Wait!\n",
      "Output seq : Warte!\n",
      "\n",
      "Input seq : Wait.\n",
      "Output seq : Warten Sie es!\n",
      "\n",
      "Input seq : Begin.\n",
      "Output seq : Halte dich!\n",
      "\n",
      "Input seq : Do it.\n",
      "Output seq : Tuten Sie Tom!\n",
      "\n",
      "Input seq : Do it.\n",
      "Output seq : Tuten Sie Tom!\n",
      "\n",
      "Input seq : Go on.\n",
      "Output seq : Geh nicht!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    input_seq = encoder_input_data[i:i+1]\n",
    "    decoded_seq = decode_sequence(input_seq)\n",
    "    print(f'Input seq : {input_texts[i]}')\n",
    "    print(f'Output seq : {decoded_seq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-gathering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-canadian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
